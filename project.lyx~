#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry true
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 2cm
\topmargin 2cm
\rightmargin 2cm
\bottommargin 2cm
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Mandril algorithm
\begin_inset Note Note
status open

\begin_layout Plain Layout
Change this to the paper title + project
\end_layout

\end_inset


\end_layout

\begin_layout Author
Gilad Hecht I.D.
 206837007
\end_layout

\begin_layout Section
Introduction
\end_layout

\begin_layout Standard
Inverse RL is an important subfield of Reinforcement Learning, in which
 an agent is trained not by its attempts and experiences alone, but by examples
 of some other agent on how to to maximize the reward for some specific
 task.
\end_layout

\begin_layout Standard
This is a benefit in two senses.
 Firstly it can assist in the training, of complicated problems where exploratio
n is hard either because of dimensionality, or because rewards are rare.
 Secondly, when rewards are hard to articulate, a demonstration can explain
 in a better manner what needs to be done, and how to do it, without formalizing
 the problem mathematically.
\end_layout

\begin_layout Standard
When set in the meta-learning framework, where tasks vary one from the other,
 but have some underlying principles, inverse RL gets more complicated.
 The learning agent must learn to extract the meta-task from the demonstrations,
 meaning that it needs to understand what reward function describes the
 expert's actions, and how they vary from meta-task to meta-task.
 In the paper
\begin_inset Foot
status open

\begin_layout Plain Layout
Learning a Prior over Intent via Meta-Inverse Reinforcement Learning
\end_layout

\end_inset

, the writers view meta-learning rather as a tool to assist IRL to train
 on examples, when the data is more general than would suit a regular IRL
 algorithm.
 Personally I'm not sure this is the best argument for using the meta-learning
 framework with IRL, as the contribution of meta-learning to the acheivements,
 as compared to other algorithms, is vague.
 In any case, I find the framework interesting, as both meta-learning and
 inverse reinforcement learning seem to me central areas of progress in
 RL.
\end_layout

\begin_layout Subsection
What is mandril
\end_layout

\begin_layout Standard
Mandril is the algorithm proposed in [
\begin_inset Note Note
status open

\begin_layout Plain Layout
Create citation for the paper
\end_layout

\end_inset

].
 It is rather a simple amalgamation of MaxEnt, and MAML.
 The idea is to use Maml, but in the meta-learning phase, instead of using
 the agent's attempts to update the agent for the task, the agent's actions
 are compared to the expert's and the inner reward function is updated in
 concordance to the expert's behavior.
 The comparison is done using a fit of the expert's behavior to a reward
 function using maximum entropy - meaning that the reward function best
 describing the expert's inner reward function, out of some set of functions,
 is the one that maximizes the entropy while also fitting statistically
 with the expert's actions.
\end_layout

\begin_layout Standard
This means that even though we have no well defined formulation of the expert's
 behavior, we will still try and replicate it in the agent's model.
\end_layout

\begin_layout Standard
Apart from this central part, the MAML algorithm remains unchanged.
\begin_inset Note Note
status open

\begin_layout Plain Layout
Enter some equations describing the algorithm
\end_layout

\end_inset


\end_layout

\begin_layout Subsection
MaxEnt
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
Write how I use the expert to learn according to the trajectory of the expert
\end_layout

\end_inset


\end_layout

\begin_layout Standard
The algorithm uses MaxEnt - to find a reward function that explains the
 expert's behavior.
 The way it works is that given a number of trajectory examples, the state
 visitation frequency is computed, using the agent's reward function to
 calculate the probabilities of the different actions using the maximum
 entropy theorem, and then updating the reward function with gradient descent.
 This continues iteratively until the change to the reward function is below
 some threshold, so that the updates to the reward are minor.
\end_layout

\begin_layout Section
Learning from partially optimal experts
\end_layout

\begin_layout Subsection
The problem space - grid-world
\end_layout

\begin_layout Standard
The problem space is a fairly simple one.
 A NxN-sized grid, with varying starting points, and some terminal point.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:example of a task"

\end_inset

example of a task
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
The goal is to arrive as quickly as possible to the goal square.
\end_layout

\begin_layout Standard
In order to make this a meta-learning problem, each meta-task has some terminal
 point with varying starting points.
 From task to task the terminal point changes.
\end_layout

\begin_layout Standard
Further on we can add additional reward locations that the agent can pass
 through to accumulate additional rewards.
\end_layout

\begin_layout Section
Experts
\end_layout

\begin_layout Standard
I started with the optimal expert that solves the problem optimally.
 This gives the agent the best possible outcome it could hope to match.
 In order to create sub-optimal experts, I use a
\begin_inset Quotes eld
\end_inset

slipping
\begin_inset Quotes erd
\end_inset

 expert which for some probability takes some random action.
 Meaning:
\begin_inset Formula 
\[
Pr(a_{expert-\epsilon}=a_{Optimal})=1-\frac{3}{4}\epsilon
\]

\end_inset


\begin_inset Formula 
\[
Pr(a_{expert-\epsilon}=a^{\prime}\in\{a\}/a_{Optimal})=\frac{1}{4}\epsilon
\]

\end_inset


\end_layout

\begin_layout Standard
We can call this an 
\begin_inset Formula $\epsilon$
\end_inset

-noisy Optimal solver.
\end_layout

\begin_layout Standard
The goal of comparing the agents trained on these experts is to ascertain
 firstly how much the agent replicates the expert, and secondly how much
 it learns the gist of the problem, and strives to improve over the expert.
\end_layout

\begin_layout Subsection
Scores
\end_layout

\begin_layout Standard
In order to grade the learned reward functions, we use an optimal solver
 for the problem.
 This solver, given the reward function for all state of the board, returns
 the optimal action for each state.
 We therefore compare the optimal actions for the Ground-Truth reward function,
 compared with the learned reward function using:
\begin_inset Formula 
\[
score=\frac{1}{N}\sum_{n\in1}^{N}\delta\left(a_{optimal},a_{mandril}\right)
\]

\end_inset

The lower the score the better.
\end_layout

\begin_layout Standard
Another option is to compare the reward functions, for instance the L2 distance
 between them.
\end_layout

\begin_layout Section
Comparing between Mandril and regular maxEnt
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
I want to show that maxent is better at training regular IRL problems (such
 as changing mazes), but that Mandril is better when it comes to real meta-learn
ing problems.
\end_layout

\end_inset


\end_layout

\begin_layout Standard
To start, I wanted to compare between mandril, with regular maxent for each
 meta-task.
 For this I ran the algorithm a couple of times for varying batch sizes.
 The outcomes can be seen in figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Scores for policies trained with Mandril, vs regular MaxEnt per task (lower is better)"
plural "false"
caps "false"
noprefix "false"

\end_inset


\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Scores for policies trained with Mandril, vs regular MaxEnt per task (lower is better)"

\end_inset

Scores for policies trained with Mandril, vs regular MaxEnt per task (lower
 is better)
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset

.
 
\begin_inset Note Note
status open

\begin_layout Plain Layout
Add description for the outcome.
\end_layout

\end_inset


\end_layout

\end_body
\end_document
